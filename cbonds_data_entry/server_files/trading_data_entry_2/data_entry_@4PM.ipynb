{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import mysql.connector\n",
    "import datetime\n",
    "import pytz\n",
    "import csv\n",
    "import smtplib,ssl\n",
    "from email import encoders\n",
    "from email.mime.base import MIMEBase\n",
    "from email.mime.multipart import MIMEMultipart\n",
    "from email.mime.text import MIMEText\n",
    "import logging as log\n",
    "log_file_path = r'D:\\\\sriram\\\\agrud\\\\cbonds_data_entry\\\\server_files\\\\trading_data_entry_2\\\\scraper_run_log.txt'\n",
    "log.basicConfig(filename = log_file_path,filemode='a',level=log.INFO)\n",
    "my_log = log.getLogger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Isin_to_masterid.json', 'r') as f:\n",
    "    isinToMasterid = json.load(f)\n",
    "with open('col_to_indicator.json', 'r', encoding='utf-8') as f:\n",
    "    colToIndicator = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "close_price_priority = ['close price', 'indicative price', 'bid (at close)', 'ask (at close)']\n",
    "stock_exchange_priority = ('Frankfurt S.E.','Stuttgart Exchange','Berlin Exchange','Dusseldorf SE','FINRA TRACE','Cbonds Estimation','Luxembourg S.E','Luxembourg S.E.','MiFID II Source 2 (APA, Post-trade reporting)','MiFID II Source 1 (APA, Post-trade reporting)','Hong Kong S.E.','SGX','US OTC Market','Other sources of prices','London S.E.','Euronext Paris','Taipei Exchange (OTC)','Nasdaq Dubai','Taipei Exchange (Trading System)','Gettex','Italian S.E. - EuroTLX','SIX','Euronext Amsterdam','FedInvest','Munich SE','Australian S.E.','NZX')\n",
    "dateColumns = ('trade date','put/Ñall date','maturity date')\n",
    "close_price_priority = ['close price', 'indicative price', 'bid (at close)', 'ask (at close)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "publish_date = datetime.datetime.today().strftime('%Y-%m-%d')\n",
    "# publish_date = '2022-02-03'\n",
    "\n",
    "path = r'D:\\\\sriram\\\\agrud\\\\cbonds_data_entry\\\\server_files\\\\zipfiles\\\\data_files\\\\'+publish_date+'\\\\'\n",
    "publish_date = (datetime.datetime.strptime(publish_date, '%Y-%m-%d').date() - datetime.timedelta(days=1)).strftime(\"%Y-%m-%d\")\n",
    "try:\n",
    "    df = pd.read_excel(path+'tradings.xls')\n",
    "except:\n",
    "    df = pd.read_csv(path+'tradings.csv', sep=',', encoding='latin')\n",
    "df['ts date'] = publish_date\n",
    "df.columns = df.columns.str.strip().str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['master id'] = df['isin-code'].map(isinToMasterid)\n",
    "df = df[df['stock exchange (eng)'].isin(stock_exchange_priority)]\n",
    "df = df.dropna(subset=['master id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['custom close price col'] = np.nan\n",
    "for col in close_price_priority :\n",
    "    if len(df.loc[df['custom close price col'].isna() & df[col].notna()]):\n",
    "        df.loc[df['custom close price col'].isna() & df[col].notna(), 'custom close price col'] = df.loc[df['custom close price col'].isna() & df[col].notna(), col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_isin = []\n",
    "for k in isinToMasterid :\n",
    "    if not df['isin-code'].isin([k]).any():\n",
    "        missing_isin.append(k)\n",
    "with open(f'{publish_date}_missing_isin.csv', mode='w',encoding=\"utf-8\",newline=\"\") as missing_file:\n",
    "    writer = csv.writer(missing_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_ALL)\n",
    "    for i in missing_isin:\n",
    "        writer.writerow([i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1460"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(missing_isin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_email(row_count=0,status=None,text=None):\n",
    "    email_user = 'agrud.scrapersmail123@gmail.com'\n",
    "    email_password = 'qwerty@123'\n",
    "    email_send = 'prince.chaturvedi@agrud.com'\n",
    "\n",
    "    subject = f\"trading data ingestion results: {publish_date}\"\n",
    "    message = MIMEMultipart()\n",
    "    message['From'] = email_user\n",
    "    message['To'] = email_send\n",
    "    message['Subject'] = subject\n",
    "\n",
    "    body = f\"Total records inserted: {row_count}\\ncronjob status: {status}\\nError:{text}\"\n",
    "\n",
    "    message.attach(MIMEText(body,'plain'))\n",
    "\n",
    "    attach_file_name = f'{publish_date}_missing_isin.csv'\n",
    "    with open(attach_file_name,'rb') as send_file:\n",
    "        payload = MIMEBase('application', 'octate-stream')\n",
    "        payload.set_payload(send_file.read())\n",
    "    print(attach_file_name)\n",
    "    encoders.encode_base64(payload) \n",
    "    payload.add_header('Content-Decomposition',f'attachment; filename={attach_file_name}')\n",
    "    message.attach(payload)\n",
    "    text = message.as_string()\n",
    "    context = ssl.create_default_context()\n",
    "    with smtplib.SMTP_SSL('smtp.gmail.com',465,context=context ) as server:\n",
    "        server.login(email_user,email_password)\n",
    "        server.sendmail(email_user,email_send.split(\",\"),text)\n",
    "    my_log.info('email sent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessedData = pd.DataFrame()\n",
    "issueData = []\n",
    "for by, groupedDf in df.groupby(by = ['master id', 'ts date']):\n",
    "    master_id = int(by[0])\n",
    "    ts_date = by[1]\n",
    "    for stock_exchange in stock_exchange_priority:\n",
    "        close_price_col = 'custom close price col'\n",
    "        stock_exchange_wise_df = groupedDf[groupedDf['stock exchange (eng)'] == stock_exchange]\n",
    "        if (len(stock_exchange_wise_df) > 0) and (stock_exchange_wise_df[close_price_col].notna().sum()):\n",
    "            preprocessedData = preprocessedData.append(stock_exchange_wise_df)\n",
    "            break\n",
    "    else:\n",
    "        preprocessedData = preprocessedData.append(groupedDf.iloc[0])\n",
    "        issueData.append([master_id, str(ts_date)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "close_price_col = 'custom close price col'\n",
    "preprocessedData.loc[preprocessedData['open price'].isna(), 'open price'] = preprocessedData.loc[preprocessedData['open price'].isna(), close_price_col]\n",
    "preprocessedData.loc[preprocessedData['maximum price'].isna(), 'maximum price'] = preprocessedData.loc[preprocessedData['maximum price'].isna(), close_price_col]\n",
    "preprocessedData.loc[preprocessedData['minimum price'].isna(), 'minimum price'] = preprocessedData.loc[preprocessedData['minimum price'].isna(), close_price_col]\n",
    "preprocessedData.loc[preprocessedData['close price'].isna(), 'close price'] = preprocessedData.loc[preprocessedData['close price'].isna(), close_price_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessedData = preprocessedData.replace([np.NaN], ['NA'])\n",
    "preprocessedData = preprocessedData.replace([pd.NaT], ['NA'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "for i, row in preprocessedData.iterrows():\n",
    "    row2 = row.to_dict()\n",
    "    masterId = row2['master id']\n",
    "    for k, v in row2.items():\n",
    "        if v == \"NA\":\n",
    "            continue\n",
    "        if k in colToIndicator:\n",
    "            try:\n",
    "                indicatorId = colToIndicator[k]\n",
    "                if type(v) == pd.Timestamp:\n",
    "                    json_data = None\n",
    "                    dataType = 2\n",
    "                    gmt = pytz.timezone(\"GMT\")\n",
    "                    value_data = gmt.localize(v).timestamp()\n",
    "                elif type(v) == float or type(v) == int or  v.isnumeric(): \n",
    "                    dataType = 0\n",
    "                    value_data = v\n",
    "                    json_data = None\n",
    "                elif v.isnumeric() == False:\n",
    "                    json_data = json.dumps({'TEXT':v})\n",
    "                    dataType = 3\n",
    "                    value_data = 0\n",
    "            except Exception as e:\n",
    "                print(k)\n",
    "                print(v)\n",
    "                print(e)\n",
    "            result.append([masterId,indicatorId,value_data,json_data,dataType,publish_date])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert query\n",
    "try:\n",
    "        db_conn = mysql.connector.connect(host='54.237.79.6',user='rentech_user',database = 'rentech_db',password='N)baegbgqeiheqfi3e9314jnEkekjb',auth_plugin='mysql_native_password')\n",
    "        cursor = db_conn.cursor()\n",
    "        sql = \"\"\"INSERT INTO `raw_data` (`id`, `master_id`, `indicator_id`, `value_data`, `json_data`, `data_type`, `ts_date`, `ts_hour`, `job_id`, `timestamp`) VALUES \n",
    "        (NULL, %s, %s, %s, %s, %s, %s, '0:0:0', 9, NOW()) ON DUPLICATE KEY UPDATE  \n",
    "        master_id = VALUES(master_id), indicator_id = VALUES(indicator_id), value_data = VALUES(value_data), json_data = VALUES(json_data),data_type = VALUES(data_type), ts_date = VALUES(ts_date) ,ts_hour = VALUES(ts_hour),\n",
    "        job_id = VALUES(job_id), batch_id = VALUES(batch_id);\"\"\"\n",
    "        cursor.executemany(sql, result)\n",
    "        rows = cursor.rowcount\n",
    "        my_log.info(f'{rows} records inserted successfully')\n",
    "        db_conn.commit()\n",
    "except Exception as e:\n",
    "        my_log.setLevel(log.ERROR)\n",
    "        my_log.error(f'Mysql Error:{e}',exc_info=True)\n",
    "        # send_email(status='Fail',text=str(e))\n",
    "finally:\n",
    "        if (db_conn.is_connected()):\n",
    "            cursor.close()\n",
    "            db_conn.close()\n",
    "            my_log.info(\"MySQL connection is closed\")\n",
    "        #     send_email(rows,status = 'Success')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c40bc86b65c221928a8dcb60bcd533d914928a478170fa7541d6a17a6588b245"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
