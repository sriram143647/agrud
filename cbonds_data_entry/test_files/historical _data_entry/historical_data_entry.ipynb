{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import mysql.connector\n",
    "import math\n",
    "import csv\n",
    "import os\n",
    "import pytz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_files = []\n",
    "file_path = r'D:\\\\sriram\\\\agrud\\\\cbonds_data_entry\\\\test_files\\\\historical_data\\\\historical_data\\\\quotes_archive\\\\'\n",
    "files = os.listdir(file_path)\n",
    "start_pt = 2200\n",
    "end_pt = 2250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('isin_to_masterid_map.json', 'r') as f:\n",
    "    isinToMasterid = json.load(f)\n",
    "with open('col_to_indicator_map.json', 'r', encoding='utf-8') as f:\n",
    "    colToIndicator = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_exchange_priority = ('Frankfurt S.E.','Stuttgart Exchange','Berlin Exchange','Dusseldorf SE','FINRA TRACE','Cbonds Estimation','Luxembourg S.E','MiFID II Source 2 (APA, Post-trade reporting)','MiFID II Source 1 (APA, Post-trade reporting)','Hong Kong S.E.','SGX','US OTC Market','Other sources of prices','London S.E.','Euronext Paris','Taipei Exchange (OTC)','Nasdaq Dubai','Taipei Exchange (Trading System)','FedInvest')\n",
    "close_price_priority = ['close price', 'indicative price', 'bid (at close)', 'ask (at close)']\n",
    "dateColumns = ('trade date','put/сall date',\"put/\\u0441all date\",'maturity date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XS1076957700\n",
      "XS1077882121\n",
      "XS1077883012\n",
      "XS1078662563\n",
      "XS1078750004\n",
      "XS1079028566\n",
      "XS1079527211\n",
      "XS1079726334\n",
      "XS1079848369\n",
      "XS1081321595\n",
      "XS1082471423\n",
      "XS1082971588\n",
      "XS1083844503\n",
      "XS1083986718\n",
      "XS1084368593\n",
      "XS1084942470\n",
      "XS1086900898\n",
      "XS1088515207\n",
      "XS1090334050\n",
      "XS1090864528\n",
      "XS1090889947\n",
      "XS1094768469\n",
      "XS1097953050\n",
      "XS1104029290\n",
      "XS1105268228\n",
      "XS1106137687\n",
      "XS1106513762\n",
      "XS1107291541\n",
      "XS1108784510\n",
      "XS1111123987\n",
      "XS1113141441\n",
      "XS1115459528\n",
      "XS1115498260\n",
      "XS1115502988\n",
      "XS1115524016\n",
      "XS1117293107\n",
      "XS1117297355\n",
      "XS1120081283\n",
      "XS1120403313\n",
      "XS1120608713\n",
      "XS1120937617\n",
      "XS1121908211\n",
      "XS1125272143\n",
      "XS1128996425\n",
      "XS1130058743\n",
      "XS1130913558\n",
      "XS1134541306\n",
      "XS1134541561\n",
      "XS1138283152\n",
      "XS1138283236\n"
     ]
    }
   ],
   "source": [
    "for file in files[start_pt:end_pt]:\n",
    "    print(file.strip('.xls'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# publish_date = \"2022-01-11\"\n",
    "# publish_date = (datetime.datetime.strptime(publish_date, '%Y-%m-%d').date() - datetime.timedelta(days=1)).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "df = pd.DataFrame()\n",
    "not_insert = []\n",
    "with open(\"not_insert_isin.csv\",\"r\") as file:\n",
    "    csvreader = csv.reader(file)\n",
    "    for row in csvreader:\n",
    "        not_insert.append(row[0])\n",
    "# count = 1\n",
    "for file in files[start_pt:end_pt]:\n",
    "# for file in files:\n",
    "    if file.strip('.xls') in not_insert:\n",
    "        continue\n",
    "    # print(file)\n",
    "    try:\n",
    "        df2 = pd.read_excel(file_path+file+'.xls')\n",
    "    except FileNotFoundError:\n",
    "        df2 = pd.read_excel(file_path+file)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    # print(count)\n",
    "    # count+=1\n",
    "    df2.columns = df2.columns.str.strip().str.lower()\n",
    "    df2['ts date'] = df2['trade date']\n",
    "    df2['master id'] = df2['isin / isin regs'].map(isinToMasterid)\n",
    "    df2 = df2[df2['stock exchange (eng)'].isin(stock_exchange_priority)]\n",
    "    df = df.append(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['master id'] = df['master id'].astype('int')\n",
    "df['custom close price col'] = np.nan\n",
    "for k in stock_exchange_priority:\n",
    "    for col in close_price_priority:\n",
    "        df.loc[(df['stock exchange (eng)']==k) & (df['custom close price col'].isna()) \n",
    "        & (df[col].notna()), 'custom close price col'] = df.loc[(df['stock exchange (eng)']==k) & (df['custom close price col'].isna()) & (df[col].notna()), col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessedData = pd.DataFrame()\n",
    "issueData = []\n",
    "i = 0\n",
    "for by, groupedDf in df.groupby(by = ['master id','ts date']):\n",
    "    # print(i)\n",
    "    master_id = int(by[0])\n",
    "    ts_date = by[1]\n",
    "    for stock_exchange in stock_exchange_priority:\n",
    "        close_price_col = 'custom close price col'\n",
    "        stock_exchange_wise_df = groupedDf[groupedDf['stock exchange (eng)'] == stock_exchange]\n",
    "        if (len(stock_exchange_wise_df) > 0) and (stock_exchange_wise_df[close_price_col].notna().sum()):\n",
    "            preprocessedData = preprocessedData.append(stock_exchange_wise_df)\n",
    "            break\n",
    "    else:\n",
    "        issueData.append([str(ts_date)])\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_dates = []\n",
    "for x in issueData:\n",
    "    missing_dates.append(x[0])\n",
    "miss_dict = {}\n",
    "miss_dict[file] = missing_dates\n",
    "file1 = open(\"isin_missing_dates.json\",mode=\"a\",encoding=\"utf-8\")\n",
    "file1.write(json.dumps(miss_dict)+'\\n')\n",
    "file1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "close_price_col = 'custom close price col'\n",
    "preprocessedData.loc[preprocessedData['open price'].isna(), 'open price'] = preprocessedData.loc[preprocessedData['open price'].isna(), close_price_col]\n",
    "preprocessedData.loc[preprocessedData['maximum price'].isna(), 'maximum price'] = preprocessedData.loc[preprocessedData['maximum price'].isna(), close_price_col]\n",
    "preprocessedData.loc[preprocessedData['minimum price'].isna(), 'minimum price'] = preprocessedData.loc[preprocessedData['minimum price'].isna(), close_price_col]\n",
    "preprocessedData.loc[preprocessedData['close price'].isna(), 'close price'] = preprocessedData.loc[preprocessedData['close price'].isna(), close_price_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessedData = preprocessedData.replace([np.NaN], ['NA'])\n",
    "preprocessedData = preprocessedData.replace([pd.NaT], ['NA'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "for i, row in preprocessedData.iterrows():\n",
    "    row2 = row.to_dict()\n",
    "    masterId = row2['master id']\n",
    "    ts_date = row2['ts date'].strftime(\"%Y-%m-%d\")\n",
    "    for k, v in row2.items():\n",
    "        if v == \"NA\":\n",
    "            continue\n",
    "        if k in colToIndicator:\n",
    "            try:\n",
    "                indicatorId = colToIndicator[k]\n",
    "                if type(v) == pd.Timestamp:\n",
    "                    json_data = None\n",
    "                    dataType = 2\n",
    "                    gmt = pytz.timezone(\"GMT\")\n",
    "                    value_data = gmt.localize(v).timestamp()\n",
    "                elif type(v) == float or type(v) == int or  v.isnumeric(): \n",
    "                    dataType = 0\n",
    "                    value_data = v\n",
    "                    json_data = None\n",
    "                elif v.isnumeric() == False:\n",
    "                    json_data = json.dumps({'TEXT':v})\n",
    "                    dataType = 3\n",
    "                    value_data = 0\n",
    "            except Exception as e:\n",
    "                print(k)\n",
    "                print(v)\n",
    "                print(e)\n",
    "            result.append([masterId,indicatorId,value_data,json_data,dataType, ts_date])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1666687"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert query\n",
    "# try:\n",
    "#     db_conn = mysql.connector.connect(host='54.237.79.6',user='rentech_user',database = 'rentech_db',password='N)baegbgqeiheqfi3e9314jnEkekjb',auth_plugin='mysql_native_password')\n",
    "#     cursor = db_conn.cursor()\n",
    "#     sql = \"\"\"INSERT INTO `raw_data` (`id`, `master_id`, `indicator_id`, `value_data`, `json_data`, `data_type`, `ts_date`, `ts_hour`, `job_id`, `timestamp`) VALUES \n",
    "#     (NULL, %s, %s, %s, %s, %s, %s, '0:0:0', 9, NOW()) ON DUPLICATE KEY UPDATE  \n",
    "#     master_id = VALUES(master_id), indicator_id = VALUES(indicator_id), value_data = VALUES(value_data), json_data = VALUES(json_data),data_type = VALUES(data_type), ts_date = VALUES(ts_date) ,ts_hour = VALUES(ts_hour),\n",
    "#     job_id = VALUES(job_id), batch_id = VALUES(batch_id);\"\"\"\n",
    "#     cursor.executemany(sql, result)\n",
    "#     print (f\"{cursor.rowcount} rows are inserted into db\")\n",
    "#     db_conn.commit()\n",
    "# except Exception as e:\n",
    "#     print (\"Mysql Error:\",e)\n",
    "# finally:\n",
    "#     if(db_conn.is_connected()):\n",
    "#             cursor.close()\n",
    "#             db_conn.close()\n",
    "#             print(\"MySQL connection is closed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 200000\n",
      "200000 rows are inserted into db\n",
      "MySQL connection is closed\n",
      "200000 400000\n",
      "200000 rows are inserted into db\n",
      "MySQL connection is closed\n",
      "400000 600000\n",
      "200000 rows are inserted into db\n",
      "MySQL connection is closed\n",
      "600000 800000\n",
      "200000 rows are inserted into db\n",
      "MySQL connection is closed\n",
      "800000 1000000\n",
      "200000 rows are inserted into db\n",
      "MySQL connection is closed\n",
      "1000000 1200000\n",
      "200000 rows are inserted into db\n",
      "MySQL connection is closed\n",
      "1200000 1400000\n",
      "200000 rows are inserted into db\n",
      "MySQL connection is closed\n",
      "1400000 1600000\n",
      "200000 rows are inserted into db\n",
      "MySQL connection is closed\n",
      "1600000 1800000\n",
      "66687 rows are inserted into db\n",
      "MySQL connection is closed\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, math.ceil(len(result)/200000) + 1):\n",
    "    start = (i-1) * 200000\n",
    "    end = i * 200000\n",
    "    print(start, end)\n",
    "    # insert query\n",
    "    try:\n",
    "        db_conn = mysql.connector.connect(host='54.237.79.6',user='rentech_user',database = 'rentech_db',password='N)baegbgqeiheqfi3e9314jnEkekjb',auth_plugin='mysql_native_password')\n",
    "        cursor = db_conn.cursor()\n",
    "        sql = \"\"\"INSERT INTO `raw_data` (`id`, `master_id`, `indicator_id`, `value_data`, `json_data`, `data_type`, `ts_date`, `ts_hour`, `job_id`, `timestamp`) VALUES \n",
    "        (NULL, %s, %s, %s, %s, %s, %s, '0:0:0', 9, NOW()) ON DUPLICATE KEY UPDATE  \n",
    "        master_id = VALUES(master_id), indicator_id = VALUES(indicator_id), value_data = VALUES(value_data), json_data = VALUES(json_data),data_type = VALUES(data_type), ts_date = VALUES(ts_date) ,ts_hour = VALUES(ts_hour),\n",
    "        job_id = VALUES(job_id), batch_id = VALUES(batch_id);\"\"\"\n",
    "        cursor.executemany(sql, result[start:end])\n",
    "        print (f\"{cursor.rowcount} rows are inserted into db\")\n",
    "        db_conn.commit()\n",
    "    except Exception as e:\n",
    "        print (\"Mysql Error:\",e,\"-\")\n",
    "    finally:\n",
    "        if(db_conn.is_connected()):\n",
    "            cursor.close()\n",
    "            db_conn.close()\n",
    "            print(\"MySQL connection is closed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c40bc86b65c221928a8dcb60bcd533d914928a478170fa7541d6a17a6588b245"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
